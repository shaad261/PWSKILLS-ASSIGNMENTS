{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4628671-a535-4ca6-96fa-5cca620f5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q1\n",
    "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.\n",
    "While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b4969-27e1-42f4-9e32-db56a5650313",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2\n",
    "The optimal K value usually found is the square root of N, where N is the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a2e4d-a408-4554-9239-8074116627b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3\n",
    "Knn Classifier: Predicts a class by using the highest majority category among its k nearest neighbors.\n",
    "\n",
    "Knn Regression: Predicts a value by using the mean of the k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74bac6-34e8-42d8-af70-0c2bf30be2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q4\n",
    "The \"curse of dimensionality\" refers to a set of challenges that arise when working with high-dimensional data in machine learning and data analysis.\n",
    "It is particularly relevant to algorithms like k-Nearest Neighbors (KNN). \n",
    "The curse of dimensionality becomes more pronounced as the number of dimensions (features) in your dataset increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18787d50-c28e-4cf7-a261-a8b0b4692a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q5\n",
    "The idea in kNN methods is to identify ‘k’ samples in the dataset that are similar or close in the space.\n",
    "Then we use these ‘k’ samples to estimate the value of the missing data points.\n",
    "Each sample’s missing values are imputed using the mean value of the ‘k’-neighbors found in the dataset.\n",
    "KNNimputer is a scikit-learn class used to fill out or predict the missing values in a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a47d58-b715-49f4-89b8-bd9e144a7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q6\n",
    "KNN Classifier:\n",
    "\n",
    "Purpose: KNN classification is used to predict the class or category that a data point belongs to based on the classes of its k-nearest neighbors.\n",
    "Output: The output of KNN classification is a class label or category.\n",
    "\n",
    "KNN Regressor:\n",
    "\n",
    "Purpose: KNN regression is used to predict a continuous numerical value for a data point based on the values of its k-nearest neighbors.\n",
    "Output: The output of KNN regression is a continuous numeric value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b1edd-bb76-4e3e-9256-033ee5022ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q7\n",
    "Advantages of KNN Algorithm:\n",
    "    Simple and Easy to Understand\n",
    "    Non-parametric- The KNN algorithm is a non-parametric algorithm, meaning that it does not make any assumptions about the underlying distribution of the data. \n",
    "    No Training Required-The KNN algorithm does not require any training process, which means that it can be used in real-time applications where data is continuously being generated.\n",
    "    Can Handle Large Datasets – The KNN algorithm can handle large datasets without suffering from the curse of dimensionality, which is a common problem in other machine learning algorithms. \n",
    "\n",
    "    \n",
    "Disadvantages of KNN Algorithm:\n",
    "    Sensitive to Outliers – The KNN algorithm can be sensitive to outliers in the data, which can significantly affect its performance. Outliers are data points that are significantly different from the rest of the data, and they can have a disproportionate impact on the KNN algorithm’s classification results\n",
    "    Computationally Expensive – The KNN algorithm can be computationally expensive, particularly for large datasets. This is because the algorithm needs to compute the distance between each test data point and every training data point, which can be time-consuming.\n",
    "    Requires Good Choice of K – The KNN algorithm requires a good choice of the K parameter, which determines the number of nearest neighbors used for classification. \n",
    "    Limited to Euclidean Distance – The KNN algorithm is limited to using the Euclidean distance metric to measure the distance between data points. This can be a disadvantage when working with non-Euclidean data, such as categorical or binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32164af9-7e22-4ced-a318-dfbdacc4a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
